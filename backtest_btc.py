# -*- coding: utf-8 -*-
"""Backtest_BTC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gdt7RxEHnlXm-5L_uzgHVe3tmDpV1wZb
"""

print('Dependencies loading......')

!pip install pyforest
from pyforest import *
import datetime, pickle, copy, warnings
!pip install cryptocompare
import cryptocompare
import requests
!pip install plotly
!pip install cufflinks
import plotly.express as px
import plotly.graph_objects as go
from time import time
from pandas import DataFrame, concat
from sklearn import metrics
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error, r2_score
from math import sqrt
!pip install ta
import ta
!pip install quandl
import quandl

print('Complete....')

# Linear model

start = time()
print('Starting program ( hourly data)....')

apiKey = "43b01c420b66888ce4c91b364647600814578c186e8604322152f44c641ebbc1"
url = "https://min-api.cryptocompare.com/data/histohour"

# BTC data
payload = {
    "api_key": apiKey,
    "fsym": "BTC",
    "tsym": "USD",
    "limit": 2000
}

result = requests.get(url, params=payload).json()

btc1 = DataFrame(result['Data'])
btc1['time'] = pd.to_datetime(btc1['time'],unit='s')
btc1.set_index('time',inplace=True)

# 2nd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "BTC",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1601632800)
}

result = requests.get(url, params=payload).json()

btc2 = DataFrame(result['Data'])
btc2['time'] = pd.to_datetime(btc2['time'],unit='s')
btc2.set_index('time',inplace=True)

# 3rd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "BTC",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1593572400)
}

result = requests.get(url, params=payload).json()

btc3 = DataFrame(result['Data'])
btc3['time'] = pd.to_datetime(btc3['time'],unit='s')
btc3.set_index('time',inplace=True)

# 4th 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "BTC",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1596571200)
}

result = requests.get(url, params=payload).json()

btc4 = DataFrame(result['Data'])
btc4['time'] = pd.to_datetime(btc4['time'],unit='s')
btc4.set_index('time',inplace=True)

# combining BTC dataframe
com1 = btc2.append(btc1)
com2 = btc3.append(com1)
btc = btc4.append(com2)
# saving btc data set
#btc.to_csv("bitcoin.csv")


# ETH DATA
payload = {
    "api_key": apiKey,
    "fsym": "ETH",
    "tsym": "USD",
    "limit": 2000
}

result = requests.get(url, params=payload).json()
eth1 = DataFrame(result['Data'])
eth1['time'] = pd.to_datetime(eth1['time'],unit='s')
eth1.set_index('time',inplace=True)

# 2nd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "ETH",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1601632800)
}

result = requests.get(url, params=payload).json()
eth2 = DataFrame(result['Data'])
eth2['time'] = pd.to_datetime(eth2['time'],unit='s')
eth2.set_index('time',inplace=True)

# 3rd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "ETH",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1593572400)
}

result = requests.get(url, params=payload).json()
eth3 = DataFrame(result['Data'])
eth3['time'] = pd.to_datetime(eth3['time'],unit='s')
eth3.set_index('time',inplace=True)

# 4th ETH 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "ETH",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1596571200)
}

result = requests.get(url, params=payload).json()

eth4 = DataFrame(result['Data'])
eth4['time'] = pd.to_datetime(eth4['time'],unit='s')
eth4.set_index('time',inplace=True)

# combining BTC dataframe
com1 = eth2.append(eth1)
com2 = eth3.append(com1)
eth = eth4.append(com2)

# saving ETH data set
#eth.to_csv("Ethereum.csv")

# LTC data
payload = {
    "api_key": apiKey,
    "fsym": "LTC",
    "tsym": "USD",
    "limit": 2000
}
result = requests.get(url, params=payload).json()
ltc1 = DataFrame(result['Data'])
ltc1['time'] = pd.to_datetime(ltc1['time'],unit='s')
ltc1.set_index('time',inplace=True)

# 2nd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "LTC",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1601632800)
}

result = requests.get(url, params=payload).json()
ltc2 = DataFrame(result['Data'])
ltc2['time'] = pd.to_datetime(ltc2['time'],unit='s')
ltc2.set_index('time',inplace=True)

# 3rd 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "LTC",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1593572400)
}

result = requests.get(url, params=payload).json()
ltc3 = DataFrame(result['Data'])
ltc3['time'] = pd.to_datetime(ltc3['time'],unit='s')
ltc3.set_index('time',inplace=True)

# 4th ETH 2000 data
payload = {
    "api_key": apiKey,
    "fsym": "ETH",
    "tsym": "USD",
    "limit": 2000,
    "toTs": (1596571200)
}

result = requests.get(url, params=payload).json()

ltc4 = DataFrame(result['Data'])
ltc4['time'] = pd.to_datetime(ltc4['time'],unit='s')
ltc4.set_index('time',inplace=True)

# combining dataframe
com1 = ltc2.append(ltc1)
com2 = ltc3.append(com1)
ltc = ltc4.append(com2)

# saving ETH data set
#ltc.to_csv("litecoin.csv")

# --Data Selection
from pandas import DataFrame, concat

df = DataFrame({'ETH': eth.close})
dataframe = concat([btc, df], axis=1)
dataframe.drop(columns = ['conversionType','conversionSymbol'], axis=1, inplace=True)

values = DataFrame(btc.close.values)
lags = 8
columns = [values]
for i in range(1,(lags + 1)):
    columns.append(values.shift(i))
dt = concat(columns, axis=1)
columns = ['Lag']
for i in range(1,(lags + 1)):
    columns.append('Lag' + str(i))
dt.columns = columns
dt.index = dataframe.index

dataframe = concat([dataframe, dt], axis=1)
#dataframe = pd.concat([dataframe, dt], axis=1)
dataframe.dropna(inplace=True)
#dataframe['day_of_month'] = dataframe.index.day
#dataframe['day_of_year'] = dataframe.index.dayofyear
dataframe['S_10'] = dataframe['close'].rolling(window=10).mean()
dataframe['Corr'] = dataframe['close'].rolling(window=10).corr(dataframe['S_10'])
dataframe['d_20'] = dataframe['close'].shift(480)
dataframe['5EMA'] = (dataframe['close'].ewm(span=5,adjust=True,ignore_na=True).mean())
dataframe['10EMA'] = (dataframe['close'].ewm(span=10,adjust=True,ignore_na=True).mean())
dataframe['20EMA'] = (dataframe['close'].ewm(span=20,adjust=True,ignore_na=True).mean())
#dataframe['48EMA'] = (dataframe['close'].ewm(span=48,adjust=True,ignore_na=True).mean())
#dataframe['96EMA'] = (dataframe['close'].ewm(span=96,adjust=True,ignore_na=True).mean())
dataframe['mean'] = (dataframe['low'] + dataframe['high'])/2
dataframe['returns'] = (dataframe['close'] - dataframe['open']) / dataframe['open'] * 100.0
dataframe['volume'] = dataframe['volumeto'] - dataframe['volumefrom']
dataframe.drop(['volumefrom', 'volumeto'], 1, inplace=True)
dataframe.dropna(inplace=True)
fcast_col = 'close' # creating label
fcast_out = int(24) # prediction for next 24 hrs
#dataframe['label'] = dataframe[fcast_col].shift(-fcast_out)
data = dataframe.copy()

dataframe = dataframe.drop(['Lag'], axis=1)
dataframe = dataframe.astype(float)
#dataframe.drop(['day_of_month','day_of_year'], 1, inplace=True)
dataframe = dataframe.sort_index(ascending=True)
dataframe.head(2)

dataframe.tail(2)

dataframe.shape

# If any of the values of percentage returns equal zero, setting them to
# a small number (stops issues with QDA model)
for i,x in enumerate(dataframe["returns"]):
    if (abs(x) < 0.0001):
        dataframe["returns"][i] = 0.0001
            
# Create the lagged percentage returns columns
for i in range(0, lags):
    dataframe["Lag%s" % str(i+1)] = dataframe["Lag%s" % str(i+1)].pct_change()*100.0
# Create the "Direction" column (+1 or -1) indicating an up/down period
dataframe.loc[:,"Direction"] = np.sign(dataframe["returns"])
dataframe.head(2)

dataframe.to_csv('btc_data.csv')

from google.colab import files
files.download('btc_data.csv')

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("btc_data (1) (2).csv")
df.set_index('time', inplace=True)
df.sort_index(ascending=True, inplace=True)
df.index = pd.to_datetime(df.index)
df.tail(2)

pip install tscv

df.dropna(inplace=True)
qdx = df.copy()
X = np.array(df.drop(['open', 'high',
                      'low', 'close', 'Direction',
                      'Lag6', 'Lag7', 'Lag8',
                       'returns'], axis=1))


from sklearn import preprocessing
X = preprocessing.scale(X)
#X_fcast_out = X[-fcast_out:]
#X = X[:-fcast_out]
#dataframe.dropna(inplace=True)
y = np.array(df['Direction'])

from tscv import GapKFold
gkcv = GapKFold(n_splits=5, gap_before=2, gap_after=1);
for train_index, test_index in gkcv.split(X, y):
    X_train, X_test = X[train_index], X[test_index];
    y_train, y_test = y[train_index], y[test_index];

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
# Create the (parametrised) models
print("Hit Rates/Confusion Matrices:\n")
models = [("LR", LogisticRegression()),
          ("LDA", LinearDiscriminantAnalysis()),
          ("QDA", QuadraticDiscriminantAnalysis()),
          ("LSVC", LinearSVC()),
          ("RSVM", SVC(C=1000000.0, cache_size=200, class_weight=None,
                       coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
                       max_iter=-1, probability=False, random_state=None,
                       shrinking=True, tol=0.001, verbose=False)),
          ("RF", RandomForestClassifier(
              n_estimators=1000, criterion='gini',
              max_depth=None, min_samples_split=2,
              min_samples_leaf=1, max_features='auto',
              bootstrap=True, oob_score=False, n_jobs=1,
              random_state=None, verbose=0))]
# iterate over the models
for m in models:
    # Train each of the models on the training set
    m[1].fit(X_train, y_train)
    # array of predictions on the test set
    pred = m[1].predict(X_test)
    # hit-rate and the confusion matrix for each model
    print("%s:\n%0.3f" % (m[0], m[1].score(X_test, y_test)))
    print("%s\n" % confusion_matrix(pred, y_test))

qdx= qdx[['open', 'high', 'low', 'close', 'volume', 'Direction']]
qdx.tail()

lda = LinearDiscriminantAnalysis().fit(X,y)
qdx['Predicted_Signal'] = lda.predict(X)
#ts = ts[['es_close']].copy()
#SPret.loc[:,'Close'] = ts.es_close
qdx.tail()

# number of trades over time for highest and second highest return strategy
print('Number of trades = ', (qdx.Predicted_Signal.diff()!=0).sum())

prices = qdx.copy()
prices.rename(columns ={
    'Predicted_Signal': 'predicted'}, inplace=True)
prices.drop(['Direction'], 1, inplace=True)
prices.tail(2)

!pip install pyfolio
import pyfolio as pf
!pip install backtrader
import backtrader as bt
from backtrader.feeds import PandasData
import warnings

OHLCV = ['open', 'high', 'low', 'close', 'volume']

# class to define the columns we will provide
class SignalData(PandasData):
    """
    Define pandas DataFrame structure
    """
    cols = OHLCV + ['predicted']

    # create lines
    lines = tuple(cols)

    # define parameters
    params = {c: -1 for c in cols}
    params.update({'datetime': None})
    params = tuple(params.items())

# define backtesting strategy class
class MLStrategy(bt.Strategy):
    params = dict(
    )
    
    def __init__(self):
        # keep track of open, close prices and predicted value in the series
        self.data_predicted = self.datas[0].predicted
        self.data_open = self.datas[0].open
        self.data_close = self.datas[0].close
        
        # keep track of pending orders/buy price/buy commission
        self.order = None
        self.price = None
        self.comm = None

    # logging function
    def log(self, txt):
        '''Logging function'''
        dt = self.datas[0].datetime.date(0).isoformat()
        print(f'{dt}, {txt}')

    def notify_order(self, order):
        if order.status in [order.Submitted, order.Accepted]:
            # order already submitted/accepted - no action required
            return

        # report executed order
        if order.status in [order.Completed]:
            if order.isbuy():
                self.log(f'BUY EXECUTED --- Price: {order.executed.price:.2f}, Cost: {order.executed.value:.2f},Commission: {order.executed.comm:.2f}'
                )
                self.price = order.executed.price
                self.comm = order.executed.comm
            else:
                self.log(f'SELL EXECUTED --- Price: {order.executed.price:.2f}, Cost: {order.executed.value:.2f},Commission: {order.executed.comm:.2f}'
                )

        # report failed order
        elif order.status in [order.Canceled, order.Margin, 
                              order.Rejected]:
            self.log('Order Failed')

        # set no pending order
        self.order = None

    def notify_trade(self, trade):
        if not trade.isclosed:
            return
        self.log(f'OPERATION RESULT --- Gross: {trade.pnl:.2f}, Net: {trade.pnlcomm:.2f}')

    # We have set cheat_on_open = True.This means that we calculated the signals on day t's close price, 
    # but calculated the number of shares we wanted to buy based on day t+1's open price.
    def next_open(self):
        if not self.position:
            if self.data_predicted > 0:
                # calculate the max number of shares ('all-in')
                size = int(self.broker.getcash() / self.datas[0].open)
                # buy order
                self.log(f'BUY CREATED --- Size: {size}, Cash: {self.broker.getcash():.2f}, Open: {self.data_open[0]}, Close: {self.data_close[0]}')
                self.buy(size=size)
        else:
            if self.data_predicted < 0:
                # sell order
                self.log(f'SELL CREATED --- Size: {self.position.size}')
                self.sell(size=self.position.size)

# instantiate SignalData class
data = SignalData(dataname=prices)

# instantiate Cerebro, add strategy, data, initial cash, commission and pyfolio for performance analysis
cerebro = bt.Cerebro(stdstats = False, cheat_on_open=True)
cerebro.addstrategy(MLStrategy)
cerebro.adddata(data)
cerebro.broker.setcash(100000.0)
cerebro.broker.setcommission(commission=0.001)
cerebro.addanalyzer(bt.analyzers.PyFolio, _name='pyfolio')

#pd.set_option('display.max_colwidth',10000)
# run the backtest
print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())
backtest_result = cerebro.run()
print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())

# Extract inputs for pyfolio
strat = backtest_result[0]
pyfoliozer = strat.analyzers.getbyname('pyfolio')
returns, positions, transactions, gross_lev = pyfoliozer.get_pf_items()
returns.name = 'Strategy'
returns.tail()

returns.head()

# get benchmark returns
benchmark_rets= dataframe['returns']
benchmark_rets.index = benchmark_rets.index.tz_localize('UTC') 
#benchmark_rets = benchmark_rets.filter(returns.index)
#benchmark_rets.name = 'BTC returns'
#benchmark_rets.tail()

# get performance statistics for strategy
pf.show_perf_stats(returns)